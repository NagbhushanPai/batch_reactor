{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4sEo_u79qqdF"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "from gym import spaces\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy\n",
        "from scipy.integrate import odeint\n",
        "import csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9gISNEmbqte_"
      },
      "outputs": [],
      "source": [
        "Ad      = 4.4e16\n",
        "Ed      = 140.06e3\n",
        "Ap      = 1.7e11/60\n",
        "Ep      = 16.9e3/0.239\n",
        "deltaHp = -82.2e3\n",
        "UA      = 33.3083 #%18.8445;\n",
        "Qc      = 650\n",
        "Qs      = 12.41e-2\n",
        "V       = 0.5\n",
        "Tc      = 27\n",
        "Tamb    = 27\n",
        "Cpc     = 4.184\n",
        "R       = 8.3145\n",
        "alpha   = 1.212827\n",
        "beta    = 0.000267\n",
        "epsilon = 0.5\n",
        "theta   = 1.25\n",
        "m1      = 450\n",
        "cp1     = 4.184\n",
        "mjCpj   = (18*4.184)+(240*0.49)\n",
        "cp2     = 187\n",
        "cp3     = 110.58 #%J/molK\n",
        "cp4     = 84.95\n",
        "m5      = 220\n",
        "cp5     = 0.49\n",
        "m6      = 7900\n",
        "cp6     = 0.49\n",
        "M0      = 0.7034\n",
        "I0      = 4.5e-3\n",
        "\n",
        "# Define Batch Reactor model\n",
        "def br(x,t,u,Ad):\n",
        "    # Inputs:\n",
        "    # Coolant flow rate\n",
        "    F = u*16.667\n",
        "\n",
        "    # States (4):\n",
        "    # Initiator\n",
        "    Ii  = x[0]\n",
        "    # Monomer\n",
        "    M  = x[1]\n",
        "    # Reactor temperature\n",
        "    Tr = x[2]\n",
        "    # Jacket temperature\n",
        "    Tj = x[3]\n",
        "\n",
        "    Ri    = Ad*Ii*(np.exp(-Ed/(R*(Tr+273.15))))\n",
        "    Rp    = Ap*(Ii**epsilon)*(M**(theta)*(np.exp(-Ep/(R*(Tr+273.15)))))\n",
        "    mrCpr = m1*cp1+ Ii*cp2*V + M*cp3*V +(M0-M)*cp4*V+ m5*cp5 + m6*cp6\n",
        "    Qpr   = alpha*(Tr-Tc)**beta\n",
        "\n",
        "    # Computing the rate of change of I, M, Tr, Tj using Differential Equations\n",
        "    dy1_dt = -Ri\n",
        "    dy2_dt = -Rp\n",
        "    dy3_dt = (Rp*V*(-deltaHp)-UA*(Tr-Tj)+Qc+Qs-Qpr)/mrCpr\n",
        "    dy4_dt = (UA*(Tr-Tj)-F*Cpc*(Tj-Tc))/mjCpj\n",
        "\n",
        "    # Return xdot:\n",
        "    xdot = np.zeros(4)\n",
        "    xdot[0] = dy1_dt\n",
        "    xdot[1] = dy2_dt\n",
        "    xdot[2] = dy3_dt\n",
        "    xdot[3] = dy4_dt\n",
        "\n",
        "    return xdot\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "6F1XdO4eq37O"
      },
      "outputs": [],
      "source": [
        "\n",
        "class BR3(gym.Env):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.action_space = spaces.Box(low=0.25, high=0.75, shape=(1,), dtype=np.float32)\n",
        "        self.observation_space = spaces.Box(low=0, high=100, shape=(2,), dtype=np.float32)\n",
        "\n",
        "        self.t = np.linspace(0, 7200, 7201)\n",
        "        self.i = 0\n",
        "\n",
        "        Tr_ref = pd.read_csv('Trajectory2.csv')\n",
        "        self.a1 = Tr_ref.values.tolist()\n",
        "        self.sp = self.a1[self.i][0]\n",
        "\n",
        "        # Initial conditions\n",
        "        self.I = 4.5e-3\n",
        "        self.M = 0.7034\n",
        "        self.Tr = 45.0\n",
        "        self.Tj = 40.0\n",
        "        self.state = self.Tr, self.sp\n",
        "\n",
        "        self.y0 = np.empty(4)\n",
        "        self.y0[0] = self.I\n",
        "        self.y0[1] = self.M\n",
        "        self.y0[2] = self.Tr\n",
        "        self.y0[3] = self.Tj\n",
        "\n",
        "        self.time_step = 7200\n",
        "\n",
        "    def step(self, action):\n",
        "        action = action[0]\n",
        "        u = action\n",
        "\n",
        "        ts = [self.t[self.i], self.t[self.i+1]]\n",
        "        y = scipy.integrate.odeint(br, self.y0, ts, args=(u, 4.4e16))\n",
        "        x = np.round(y, decimals=4)\n",
        "\n",
        "        self.I = x[-1][0]\n",
        "        self.M = x[-1][1]\n",
        "        self.Tr = x[-1][2]\n",
        "        self.Tj = x[-1][3]\n",
        "\n",
        "        self.y0 = np.array([self.I, self.M, self.Tr, self.Tj])\n",
        "\n",
        "        # Data saving with overwrite logic\n",
        "        data = [self.sp, self.Tr, self.Tj, action]\n",
        "        mode = 'w' if self.i == 0 else 'a'\n",
        "        with open('data.csv', mode, newline='') as file:\n",
        "            writer = csv.writer(file)\n",
        "            if self.i == 0:\n",
        "                writer.writerow(['Setpoint', 'Reactor Temperature', 'Jacket Temperature', 'Action'])\n",
        "            writer.writerow(data)\n",
        "\n",
        "        self.i += 1\n",
        "        if self.i < len(self.a1):\n",
        "            self.sp = self.a1[self.i][0]\n",
        "\n",
        "        difference = self.sp - self.Tr\n",
        "        error = abs(difference)\n",
        "\n",
        "        # Reward logic\n",
        "        if error <= 0.5:\n",
        "            self.reward = +100\n",
        "        elif error <= 1:\n",
        "            self.reward = +50\n",
        "        elif error <= 3:\n",
        "            self.reward = +25\n",
        "        elif error <= 4:\n",
        "            self.reward = +10\n",
        "        else:\n",
        "            self.reward = -100\n",
        "\n",
        "        done = self.i >= self.time_step\n",
        "        info = {}\n",
        "\n",
        "        self.state = self.Tr, self.sp\n",
        "        return self.state, self.reward, done, info\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        self.I = 4.5e-3\n",
        "        self.M = 0.7034\n",
        "        self.Tr = 45.0\n",
        "        self.Tj = 40.0\n",
        "        self.i = 0\n",
        "\n",
        "        self.sp = self.a1[self.i][0]\n",
        "        self.state = (self.Tr, self.sp)\n",
        "\n",
        "        self.y0 = np.array([self.I, self.M, self.Tr, self.Tj])\n",
        "        self.prev_action = 0.5  # Initialize previous action\n",
        "\n",
        "        return self.state, {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# GPU/CPU setup\n",
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating actor network...\n",
            "Actor network created successfully\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/5000 [01:21<?, ?it/s]\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'DDPG' object has no attribute 'writer'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 242\u001b[39m\n\u001b[32m    240\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    241\u001b[39m     ddpg = DDPG()\n\u001b[32m--> \u001b[39m\u001b[32m242\u001b[39m     rewards = \u001b[43mddpg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    244\u001b[39m     \u001b[38;5;66;03m# Plot results\u001b[39;00m\n\u001b[32m    245\u001b[39m     plt.plot(rewards)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 197\u001b[39m, in \u001b[36mDDPG.train\u001b[39m\u001b[34m(self, episodes)\u001b[39m\n\u001b[32m    194\u001b[39m rewards.append(episode_reward)\n\u001b[32m    196\u001b[39m \u001b[38;5;66;03m# Log to TensorBoard\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwriter\u001b[49m.add_scalar(\u001b[33m'\u001b[39m\u001b[33mTraining/Episode_Reward\u001b[39m\u001b[33m'\u001b[39m, episode_reward, episode)\n\u001b[32m    198\u001b[39m \u001b[38;5;28mself\u001b[39m.writer.add_scalar(\u001b[33m'\u001b[39m\u001b[33mTraining/Noise_STD\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28mself\u001b[39m.noise_std, episode)\n\u001b[32m    200\u001b[39m \u001b[38;5;66;03m# Save best model\u001b[39;00m\n",
            "\u001b[31mAttributeError\u001b[39m: 'DDPG' object has no attribute 'writer'"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from copy import deepcopy\n",
        "from collections import namedtuple\n",
        "from collections import deque  # You'll need this too for the ReplayBuffer class\n",
        "import torch.nn.functional as F  # This is also needed for F.mse_loss\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "\n",
        "# --- Same imports, device, etc. as you wrote ---\n",
        "\n",
        "from __main__ import BR3  # Or import your BR3 class properly\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, obs_dim=2, action_dim=1):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(obs_dim, 256),\n",
        "            nn.LayerNorm(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.LayerNorm(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, action_dim),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "        \n",
        "    def forward(self, state):\n",
        "        action = self.net(state) * 0.25 + 0.5  # Scale to [0.25, 0.75]\n",
        "        return action\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, obs_dim=2, action_dim=1):  # Change obs_dim to 2\n",
        "        super().__init__()\n",
        "        self.state_net = nn.Sequential(\n",
        "            nn.Linear(obs_dim, 256),\n",
        "            nn.LayerNorm(256),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.action_net = nn.Sequential(\n",
        "            nn.Linear(action_dim, 128),\n",
        "            nn.LayerNorm(128),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.q_net = nn.Sequential(\n",
        "            nn.Linear(256 + 128, 128),\n",
        "            nn.LayerNorm(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "\n",
        "        \n",
        "    def forward(self, state, action):\n",
        "        s_feat = self.state_net(state)\n",
        "        a_feat = self.action_net(action)\n",
        "        return self.q_net(torch.cat([s_feat, a_feat], dim=-1))\n",
        "\n",
        "# ========================\n",
        "# IMPROVED REPLAY BUFFER\n",
        "# ========================\n",
        "Transition = namedtuple('Transition', \n",
        "                       ('state', 'action', 'reward', 'next_state', 'done'))\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity=1e6):\n",
        "        self.buffer = deque(maxlen=int(capacity))\n",
        "        \n",
        "    def push(self, *args):\n",
        "        self.buffer.append(Transition(*args))\n",
        "        \n",
        "    def sample(self, batch_size):\n",
        "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
        "        states, actions, rewards, next_states, dones = zip(*[self.buffer[i] for i in indices])\n",
        "        return (torch.FloatTensor(np.array(states)),\n",
        "                torch.FloatTensor(np.array(actions)),\n",
        "                torch.FloatTensor(np.array(rewards)).unsqueeze(-1),\n",
        "                torch.FloatTensor(np.array(next_states)),\n",
        "                torch.FloatTensor(np.array(dones)).unsqueeze(-1))\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "# ========================\n",
        "# MODIFIED TRAINING LOOP\n",
        "# ========================\n",
        "class DDPG:\n",
        "    def __init__(self):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        try:\n",
        "        # Networks\n",
        "            print(\"Creating actor network...\")\n",
        "            self.actor = Actor(obs_dim=2, action_dim=1).to(self.device)\n",
        "            print(\"Actor network created successfully\")\n",
        "            self.critic = Critic(obs_dim=2, action_dim=1).to(self.device)\n",
        "            self.target_actor = Actor(obs_dim=2, action_dim=1).to(self.device).eval()\n",
        "            self.target_critic = Critic(obs_dim=2, action_dim=1).to(self.device).eval()\n",
        "        except Exception as e:\n",
        "            print(f\"Error during initialization: {e}\")\n",
        "        \n",
        "        # Optimizers\n",
        "        self.actor_optim = optim.Adam(self.actor.parameters(), lr=1e-4)\n",
        "        self.critic_optim = optim.Adam(self.critic.parameters(), lr=1e-3)\n",
        "        \n",
        "        # Hyperparameters\n",
        "        self.gamma = 0.99\n",
        "        self.tau = 0.01  # For target network updates\n",
        "        self.batch_size = 4096\n",
        "        self.noise_std = 0.1\n",
        "        self.noise_decay = 0.9995\n",
        "        self.grad_clip = 1.0\n",
        "        \n",
        "        # Replay buffer\n",
        "        self.buffer = ReplayBuffer(1e6)\n",
        "        \n",
        "    def update(self):\n",
        "        if len(self.buffer) < self.batch_size:\n",
        "            return\n",
        "        \n",
        "        # Sample batch\n",
        "        states, actions, rewards, next_states, dones = self.buffer.sample(self.batch_size)\n",
        "        states = states.to(self.device)\n",
        "        actions = actions.to(self.device)\n",
        "        rewards = rewards.to(self.device)\n",
        "        next_states = next_states.to(self.device)\n",
        "        dones = dones.to(self.device)\n",
        "        \n",
        "        # Critic loss\n",
        "        with torch.no_grad():\n",
        "            target_actions = self.target_actor(next_states)\n",
        "            target_q = self.target_critic(next_states, target_actions)\n",
        "            target_q = rewards + (1 - dones) * self.gamma * target_q\n",
        "            \n",
        "        current_q = self.critic(states, actions)\n",
        "        critic_loss = F.mse_loss(current_q, target_q)\n",
        "        \n",
        "        # Critic optimization\n",
        "        self.critic_optim.zero_grad()\n",
        "        critic_loss.backward()\n",
        "        nn.utils.clip_grad_norm_(self.critic.parameters(), self.grad_clip)  # NEW\n",
        "        self.critic_optim.step()\n",
        "        \n",
        "        # Actor loss\n",
        "        actor_actions = self.actor(states)\n",
        "        actor_loss = -self.critic(states, actor_actions).mean()\n",
        "        \n",
        "        # Actor optimization\n",
        "        self.actor_optim.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        nn.utils.clip_grad_norm_(self.actor.parameters(), self.grad_clip)  # NEW\n",
        "        self.actor_optim.step()\n",
        "        \n",
        "        # Soft updates\n",
        "        with torch.no_grad():\n",
        "            for t_param, param in zip(self.target_actor.parameters(), self.actor.parameters()):\n",
        "                t_param.data.copy_(self.tau * param.data + (1 - self.tau) * t_param.data)\n",
        "            for t_param, param in zip(self.target_critic.parameters(), self.critic.parameters()):\n",
        "                t_param.data.copy_(self.tau * param.data + (1 - self.tau) * t_param.data)\n",
        "                    \n",
        "    \n",
        "        \n",
        "    def train(self, episodes=5000):\n",
        "        env = BR3()\n",
        "        rewards = []\n",
        "        best_reward = -np.inf\n",
        "        \n",
        "        for episode in tqdm(range(episodes)):\n",
        "            state_info = env.reset()\n",
        "            state = state_info[0]  # Extract just the state from the tuple\n",
        "            episode_reward = 0\n",
        "            done = False\n",
        "            \n",
        "            while not done:\n",
        "                # Decaying exploration noise\n",
        "                with torch.no_grad():\n",
        "                    action = self.actor(torch.FloatTensor(state).to(self.device)).cpu().numpy()\n",
        "                noise = np.random.normal(0, self.noise_std, size=action.shape)\n",
        "                action = np.clip(action + noise, 0.25, 0.75)\n",
        "                \n",
        "                # Environment step\n",
        "                next_state, reward, done, _ = env.step(action)\n",
        "                self.buffer.push(state, action, reward, next_state, done)\n",
        "                state = next_state\n",
        "                episode_reward += reward\n",
        "                \n",
        "                # Update networks\n",
        "                self.update()\n",
        "                \n",
        "            # Decay noise\n",
        "            self.noise_std *= self.noise_decay\n",
        "            rewards.append(episode_reward)\n",
        "            \n",
        "            # Log to TensorBoard\n",
        "            self.writer.add_scalar('Training/Episode_Reward', episode_reward, episode)\n",
        "            self.writer.add_scalar('Training/Noise_STD', self.noise_std, episode)\n",
        "            \n",
        "            # Save best model\n",
        "            if episode_reward > best_reward:\n",
        "                best_reward = episode_reward\n",
        "                torch.save(self.actor.state_dict(), \"best_actor.pth\")\n",
        "                \n",
        "            # Evaluation every 100 episodes\n",
        "            if episode % 100 == 0:\n",
        "                eval_reward = self.evaluate()\n",
        "                self.writer.add_scalar('Evaluation/Reward', eval_reward, episode)\n",
        "                print(f\"Episode {episode} | Train: {episode_reward:.1f} | Eval: {eval_reward:.1f} | Noise: {self.noise_std:.4f}\")\n",
        "        \n",
        "        self.writer.close()\n",
        "        return rewards\n",
        "\n",
        "\n",
        "    \n",
        "    def evaluate(self, n_episodes=3):\n",
        "        total_reward = 0\n",
        "        env = BR3()\n",
        "    \n",
        "        for _ in range(n_episodes):\n",
        "            state_info = env.reset()\n",
        "            state = state_info[0]  # Extract just the state from the tuple\n",
        "            done = False\n",
        "            episode_reward = 0\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                while not done:\n",
        "                    action = self.actor(torch.FloatTensor(state).to(self.device)).cpu().numpy()\n",
        "                    next_state_info, reward, done, _ = env.step(action)\n",
        "                    state = next_state_info  # No need to unpack here as step() returns just the state\n",
        "                    episode_reward += reward\n",
        "                    \n",
        "            total_reward += episode_reward\n",
        "        \n",
        "        return total_reward / n_episodes\n",
        "\n",
        "# ========================\n",
        "# EXECUTION\n",
        "# ========================\n",
        "if __name__ == \"__main__\":\n",
        "    ddpg = DDPG()\n",
        "    rewards = ddpg.train()\n",
        "    \n",
        "    # Plot results\n",
        "    plt.plot(rewards)\n",
        "    plt.xlabel(\"Episode\")\n",
        "    plt.ylabel(\"Total Reward\")\n",
        "    plt.title(\"Training Progress\")\n",
        "    plt.savefig(\"training_progress.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Check device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Dummy model (increase layer sizes if needed)\n",
        "class TestModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TestModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(2, 512)  # Increased layer size\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self, s):\n",
        "        x = self.relu(self.fc1(s))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.tanh(self.fc3(x)) * 0.25 + 0.5\n",
        "        return x\n",
        "\n",
        "# Initialize model and move to GPU\n",
        "model = TestModel().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Get total VRAM\n",
        "total_vram = torch.cuda.get_device_properties(device).total_memory\n",
        "max_vram_usage = total_vram * 0.9  # 90% of total VRAM\n",
        "\n",
        "# Start testing batch sizes\n",
        "batch_size = 256\n",
        "max_batch_size = batch_size\n",
        "while True:\n",
        "    try:\n",
        "        # Allocate a batch of input tensors\n",
        "        dummy_input = torch.randn(batch_size, 2).to(device)\n",
        "        dummy_target = torch.randn(batch_size, 1).to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        output = model(dummy_input)\n",
        "        loss = criterion(output, dummy_target)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Check VRAM usage\n",
        "        used_vram = torch.cuda.memory_allocated(device)\n",
        "        print(f\"Batch Size: {batch_size}, VRAM Used: {used_vram / (1024**2):.2f} MB\")\n",
        "\n",
        "        # If memory usage is safe, increase batch size\n",
        "        if used_vram < max_vram_usage:\n",
        "            max_batch_size = batch_size\n",
        "            batch_size *= 2  # Double the batch size\n",
        "        else:\n",
        "            break  # Stop when we exceed safe VRAM usage\n",
        "\n",
        "    except RuntimeError as e:\n",
        "        print(f\"Out of memory error at batch size {batch_size}: {e}\")\n",
        "        break\n",
        "\n",
        "print(f\"ðŸš€ Maximum stable batch size: {max_batch_size}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
